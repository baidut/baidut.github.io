{"meta":{"title":"BitBlog","subtitle":"1 Bit = 1.1102e-16 Pebibyte","description":"Stay Hungry,Stay Foolish","author":"Zhenqiang Ying","url":"https://baidut.github.io"},"pages":[{"title":"Publications","date":"2017-05-20T10:18:38.000Z","updated":"2017-05-20T10:19:50.825Z","comments":true,"path":"publications/index.html","permalink":"https://baidut.github.io/publications/index.html","excerpt":"","text":"2017Z. Ying, G. Li, S. Wen, and G. TanORGB: offset correction in rgb color space for illumination-robust image processingICASSP 2017 Poster 2016Z. Ying, G. Li, X. Zang, R. Wang, and W. WangA novel shadow-free feature extractor for real-time road detectionACM MM 2016 Z. Ying and G. LiRobust lane marking detection using boundary-based inverse perspective mappingICASSP 2016 PDF Poster 2015Z. Ying, G. Li, and G. TanAn illumination-robust approach for feature-based road detectionISM 2015 PDF Code"},{"title":"","date":"2017-09-06T02:28:36.512Z","updated":"2017-09-06T02:28:36.512Z","comments":true,"path":"about/GPA.html","permalink":"https://baidut.github.io/about/GPA.html","excerpt":"","text":"4-3*(100-92.06)^2/1600 Academic grading in China"},{"title":"About Me","date":"2017-05-20T10:28:52.000Z","updated":"2017-05-21T01:17:57.831Z","comments":true,"path":"about/index-old.html","permalink":"https://baidut.github.io/about/index-old.html","excerpt":"","text":"Download Resume Short Curriculum Vitae Since 08.2015 - master student at Peking University, China 08.2011 - 07.2015 - student at Dalian University of Technology, China Awards, Grants, Scholarships2016 Merit Student Pacesetter, Peking University Exceptional Award for Academic innovation, Peking University National Scholarship, Department of Education of China 2015 Outstanding Undergraduate, Dalian University of Technology Research interestsMy research interests are in Computer Vision and Machine Learning.I am especially interested in “physics-based” and “bio-inspired” approaches for computer vision tasks. Publications2017Z. Ying, G. Li, S. Wen, and G. TanORGB: offset correction in rgb color space for illumination-robust image processingICASSP 2017 Poster 2016Z. Ying, G. Li, X. Zang, R. Wang, and W. WangA novel shadow-free feature extractor for real-time road detectionACM MM 2016 Z. Ying and G. LiRobust lane marking detection using boundary-based inverse perspective mappingICASSP 2016 PDF Poster 2015Z. Ying, G. Li, and G. TanAn illumination-robust approach for feature-based road detectionISM 2015 PDF Code"},{"title":"categories","date":"2017-05-20T14:04:34.000Z","updated":"2017-09-04T06:55:24.201Z","comments":false,"path":"categories/index.html","permalink":"https://baidut.github.io/categories/index.html","excerpt":"","text":""},{"title":"Zhenqiang Ying's Resume","date":"2017-09-04T05:06:11.000Z","updated":"2017-09-06T11:33:04.080Z","comments":true,"path":"about/index.html","permalink":"https://baidut.github.io/about/index.html","excerpt":"","text":"PDF Download Resume Research Interests Computer Vision Computer Graphics Artificial Intelligence Education Since 08.2015M.S. in Computer Science @ Peking UniversityAdvisor: Prof. Ge Li, GPA: 3.88/4.00, Ranking: 1/77 08/2011 - 07/2015B.S. in Computer Science @ Dalian University of TechnologyGPA: 3.66/4.00, Ranking: 4/90 PublicationsGoogle Scholar | ResearchGate | dblp | ORCID Z. Ying, G. Li, X. Zang, R. Wang, and W. Wang. “A novel shadow-free feature extractor for real-time road detection”. In Proceedings of the 2016 ACM on Multimedia Conference, pages 611-615, October 2016. Z. Ying, G. Li, S. Wen, and G. Tan. “ORGB: offset correction in rgb color space for illumination-robust image processing”. In Acoustics, Speech and Signal Processing (ICASSP), pages 307-319, March 2017. Z. Ying, G. Li, Y. Ren, R. Wang, and W. Wang, “A New Low-Light Image Enhancement Algorithm using Camera Response Model”. In International Conference on Computer Vision Workshop (ICCV Workshop), in press. Z. Ying and G. Li. “Robust lane marking detection using boundary-based inverse perspective mapping”. In Acoustics, Speech and Signal Processing (ICASSP), pages 1921-1925, March 2016. Z. Ying, G. Li, Y. Ren, R. Wang, and W. Wang, “A new image contrast enhancement algorithm using exposure fusion framework”. In International Conference on Computer Analysis of Images and Patterns (CAIP), pages 36-46, August 2017. N. Li, D. Xu, Z. Ying, Z. Li, and G. Li, “Searching action proposals via spatial actionness estimation and temporal path inference and tracking”. In Asian Conference on Computer Vision (ACCV), pages 384-399, November 2016. X. Yu, G. Li, Z. Ying, and X. Guo, “A new shadow removal method using color-lines”. In International Conference on Computer Analysis of Images and Patterns (CAIP), pages 307–319, August 2017. Y. Ren, G. Li, Z. Ying, and X. Guo, “A reduced-reference color distortion metric for enhanced low-light images”. In Visual Communications and Image Processing (VCIP), in press. W. Zhang, G. Li, and Z. Ying, “A new underwater image enhancing method via color correction and illumination adjustment”. In Visual Communications and Image Processing (VCIP), in press. Z. Ying, G. Li, and G. Tan, “An illumination-robust approach for feature-based road detection”. In Visual Communications and Image Processing (VCIP), pages 278–281, December 2015. Experience Since 08.2015 - Peking University Digital Media R&amp;D Center The First Place @ Symposium on Research and Application in Computer Vision (RACV) 09.2012 - 08.2015 - School of Innovation and Entrepreneurship of DUT Best Creativity Project @ National Conference on Undergraduate Innovation and Entrepreneurship Honorable Mention @ International Interdisciplinary Contest in Modeling The Second Prize @ National Undergraduate Electronics Design Contest Selected Awards 2016 Merit Student Pacesetter, Peking University Exceptional Award for Academic innovation, Peking University National Scholarship, Department of Education of China 2015 Outstanding Undergraduate, Dalian University of Technology Science and Technology Innovation Pacesetter, Dalian University of Technology"},{"title":"tags","date":"2017-05-20T12:13:41.000Z","updated":"2017-09-04T06:55:02.872Z","comments":false,"path":"tags/index.html","permalink":"https://baidut.github.io/tags/index.html","excerpt":"","text":""},{"title":"Zhenqiang Ying's Publications","date":"2017-09-04T05:06:11.000Z","updated":"2017-09-06T10:32:46.036Z","comments":true,"path":"about/publications.html","permalink":"https://baidut.github.io/about/publications.html","excerpt":"","text":"Google Scholar | ResearchGate | dblp | ORCID NOTE: This page is out-of-date. The complete list is coming soon. Z. Ying, G. Li, X. Zang, R. Wang, and W. WangA novel shadow-free feature extractor for real-time road detectionACM MM 2016 Z. Ying, G. Li, S. Wen, and G. TanORGB: offset correction in rgb color space for illumination-robust image processingICASSP 2017 Poster Z. Ying and G. LiRobust lane marking detection using boundary-based inverse perspective mappingICASSP 2016 PDF Poster Z. Ying, G. Li, and G. TanAn illumination-robust approach for feature-based road detectionISM 2015 PDF Code"}],"posts":[{"title":"A Reduced-reference Color Distortion Measurement Scheme for Enhanced Low-light Images","slug":"vcip2017color","date":"2017-08-31T16:00:00.000Z","updated":"2017-09-04T14:26:13.447Z","comments":true,"path":"2017/09/01/vcip2017color/","link":"","permalink":"https://baidut.github.io/2017/09/01/vcip2017color/","excerpt":"Low-light image enhancement algorithms can improve the subjective visual quality of low-light images and support the extraction of valuable information for some computer vision techniques. Although many low-light image enhancement algorithms have been developed in recent years, the assessment methods for enhanced images are still open topics. In this paper, we use multi-exposure image sequences and a camera response model to calculate the color distortion of enhanced images. Specifically, we first select the useful information from a multi-exposure image sequence to form a reference image based on the illumination of the enhanced image. Then, we calculate the exposure ratio map between the reference image and the enhanced image for each color channel using the camera response model. Finally, the color distortion is calculated based on the difference between three color channel exposure ratios. Experiment results show that the Pearson’s linear correction coefficient (PLCC) and Spearman’s rank correlation coefficient (SRCC) between the results of our reduced-reference method and that of the full-reference CIE Lab color difference method (∆Eab ) are close to 1. Meanwhile, our method is significantly superior to the existing enhanced image color metrics.","text":"Low-light image enhancement algorithms can improve the subjective visual quality of low-light images and support the extraction of valuable information for some computer vision techniques. Although many low-light image enhancement algorithms have been developed in recent years, the assessment methods for enhanced images are still open topics. In this paper, we use multi-exposure image sequences and a camera response model to calculate the color distortion of enhanced images. Specifically, we first select the useful information from a multi-exposure image sequence to form a reference image based on the illumination of the enhanced image. Then, we calculate the exposure ratio map between the reference image and the enhanced image for each color channel using the camera response model. Finally, the color distortion is calculated based on the difference between three color channel exposure ratios. Experiment results show that the Pearson’s linear correction coefficient (PLCC) and Spearman’s rank correlation coefficient (SRCC) between the results of our reduced-reference method and that of the full-reference CIE Lab color difference method (∆Eab ) are close to 1. Meanwhile, our method is significantly superior to the existing enhanced image color metrics.","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"image processing","slug":"image-processing","permalink":"https://baidut.github.io/tags/image-processing/"},{"name":"image Enhancement","slug":"image-Enhancement","permalink":"https://baidut.github.io/tags/image-Enhancement/"}]},{"title":"A New Low-Light Image Enhancement Algorithm using Camera Response Model","slug":"iccvw2017","date":"2017-08-31T16:00:00.000Z","updated":"2017-09-05T07:32:29.168Z","comments":true,"path":"2017/09/01/iccvw2017/","link":"","permalink":"https://baidut.github.io/2017/09/01/iccvw2017/","excerpt":"Low-light images are not conducive to human observation and computer vision algorithms due to their low visibility. To solve this problem, many image enhancement techniques have been proposed. However, existing techniques inevitably introduce color and lightness distortion when increasing visibility. To lower the distortion, we propose a novel enhancement method using the response characteristics of cameras. First, we investigate the relationship between two images with different exposures to obtain an accurate camera response model. Then we borrow the illumination estimation techniques to estimate the exposure ratio map. Finally, we use our camera response model to adjust each pixel to its desired exposure according to the estimated exposure ratio map. Experiments show that our method can obtain enhancement results with less color and lightness distortion compared to several state-of-the-art methods.","text":"Low-light images are not conducive to human observation and computer vision algorithms due to their low visibility. To solve this problem, many image enhancement techniques have been proposed. However, existing techniques inevitably introduce color and lightness distortion when increasing visibility. To lower the distortion, we propose a novel enhancement method using the response characteristics of cameras. First, we investigate the relationship between two images with different exposures to obtain an accurate camera response model. Then we borrow the illumination estimation techniques to estimate the exposure ratio map. Finally, we use our camera response model to adjust each pixel to its desired exposure according to the estimated exposure ratio map. Experiments show that our method can obtain enhancement results with less color and lightness distortion compared to several state-of-the-art methods.","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"image processing","slug":"image-processing","permalink":"https://baidut.github.io/tags/image-processing/"},{"name":"image Enhancement","slug":"image-Enhancement","permalink":"https://baidut.github.io/tags/image-Enhancement/"}]},{"title":"A New Underwater Image Enhancing Method via Color Correction and Illumination Adjustment","slug":"vcip2017underwater","date":"2017-08-31T16:00:00.000Z","updated":"2017-09-04T14:22:57.146Z","comments":true,"path":"2017/09/01/vcip2017underwater/","link":"","permalink":"https://baidut.github.io/2017/09/01/vcip2017underwater/","excerpt":"Since the light will be absorbed and scattered when travels in water, underwater imaging exists three major difficulties, including color cast, under-exposure, and fuzz. The solutions to overcome those issues are important for the exploration of the ocean. In this paper, we propose a new algorithm for improving the quality of underwater images. The algorithm is composed of two components: color correction and illumination adjustment. First, we use an efficient color enhancement method to solve the color cast. Then, based on Retinex model, we make the illumination adjustment, mainly extracting the illumination map and implementing gamma correction on it successively. Experimental results show that visual performance of our method outperforms that of other methods, and processing complexity is relatively simpler.","text":"Since the light will be absorbed and scattered when travels in water, underwater imaging exists three major difficulties, including color cast, under-exposure, and fuzz. The solutions to overcome those issues are important for the exploration of the ocean. In this paper, we propose a new algorithm for improving the quality of underwater images. The algorithm is composed of two components: color correction and illumination adjustment. First, we use an efficient color enhancement method to solve the color cast. Then, based on Retinex model, we make the illumination adjustment, mainly extracting the illumination map and implementing gamma correction on it successively. Experimental results show that visual performance of our method outperforms that of other methods, and processing complexity is relatively simpler. vcip2017underwater","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"image processing","slug":"image-processing","permalink":"https://baidut.github.io/tags/image-processing/"},{"name":"image enhancement","slug":"image-enhancement","permalink":"https://baidut.github.io/tags/image-enhancement/"}]},{"title":"A New Image Contrast Enhancement Algorithm using Exposure Fusion Framework","slug":"caip2017fuse2","date":"2017-08-21T16:00:00.000Z","updated":"2017-09-04T08:05:56.307Z","comments":true,"path":"2017/08/22/caip2017fuse2/","link":"","permalink":"https://baidut.github.io/2017/08/22/caip2017fuse2/","excerpt":"Low-light images are not conducive to human observation and computer vision algorithms due to their low visibility. Although many image enhancement techniques have been proposed to solve this problem, existing methods inevitably introduce contrast under- and over-enhancement. In this paper, we propose an exposure fusion framework and an enhancement algorithm to provide an accurate contrast enhancement.","text":"Low-light images are not conducive to human observation and computer vision algorithms due to their low visibility. Although many image enhancement techniques have been proposed to solve this problem, existing methods inevitably introduce contrast under- and over-enhancement. In this paper, we propose an exposure fusion framework and an enhancement algorithm to provide an accurate contrast enhancement. Specifically, we first design the weight matrix for image fusion using illumination estimation techniques. Then we introduce our camera response model to synthesize multi-exposure images. Next, we find the best exposure ratio so that the synthetic image is well-exposed in the regions where the original image under-exposed. Finally, the input image and the synthetic image are fused according to the weight matrix to obtain the enhancement result. Experiments show that our method can obtain results with less contrast and lightness distortion compared to that of several state-of-the-art methods. 12345678@inproceedings&#123;ying2017new, title=&#123;A New Image Contrast Enhancement Algorithm Using Exposure Fusion Framework&#125;, author=&#123;Ying, Zhenqiang and Li, Ge and Ren, Yurui and Wang, Ronggang and Wang, Wenmin&#125;, booktitle=&#123;International Conference on Computer Analysis of Images and Patterns&#125;, pages=&#123;36--46&#125;, year=&#123;2017&#125;, organization=&#123;Springer&#125;&#125; Source code and supplemental materials can be found at our project website Draft version of the paper can be found at ResearchGate:","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"image processing","slug":"image-processing","permalink":"https://baidut.github.io/tags/image-processing/"},{"name":"image enhancement","slug":"image-enhancement","permalink":"https://baidut.github.io/tags/image-enhancement/"}]},{"title":"A New Shadow Removal Method using Color-Lines","slug":"caip2017deshadow","date":"2017-08-21T16:00:00.000Z","updated":"2017-09-04T08:05:53.962Z","comments":true,"path":"2017/08/22/caip2017deshadow/","link":"","permalink":"https://baidut.github.io/2017/08/22/caip2017deshadow/","excerpt":"In this paper, we present a novel method for single-image shadow removal. From the observation of images with shadow, we find that the pixels from the object with same material will form a line in the RGB color space as illumination changes. Besides, we find these lines do not cross with the origin due to the effect of ambient light. Thus, we establish an offset correction relationship to remove the effect of ambient light. Then we derive a linear shadow image model to perform color-line identification. With the linear model, our shadow removal method is proposed as following. First, perform color-line clustering and illumination estimation. Second, use an on-the-fly learning method to detect umbra and penumbra. Third, estimate the shadow scale by the statistics of shadow-free regions. Finally, refine the shadow scale by illumination optimization. Our method is simple and effective for producing high-quality shadow-free images and has the ability for processing scenes with rich texture types and non-uniform shadows.","text":"In this paper, we present a novel method for single-image shadow removal. From the observation of images with shadow, we find that the pixels from the object with same material will form a line in the RGB color space as illumination changes. Besides, we find these lines do not cross with the origin due to the effect of ambient light. Thus, we establish an offset correction relationship to remove the effect of ambient light. Then we derive a linear shadow image model to perform color-line identification. With the linear model, our shadow removal method is proposed as following. First, perform color-line clustering and illumination estimation. Second, use an on-the-fly learning method to detect umbra and penumbra. Third, estimate the shadow scale by the statistics of shadow-free regions. Finally, refine the shadow scale by illumination optimization. Our method is simple and effective for producing high-quality shadow-free images and has the ability for processing scenes with rich texture types and non-uniform shadows. 12345678@inproceedings&#123;yu2017new, title=&#123;A New Shadow Removal Method Using Color-Lines&#125;, author=&#123;Yu, Xiaoming and Li, Ge and Ying, Zhenqiang and Guo, Xiaoqiang&#125;, booktitle=&#123;International Conference on Computer Analysis of Images and Patterns&#125;, pages=&#123;307--319&#125;, year=&#123;2017&#125;, organization=&#123;Springer&#125;&#125; Draft version of the paper can be found at ResearchGate:","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"image processing","slug":"image-processing","permalink":"https://baidut.github.io/tags/image-processing/"},{"name":"shadow removal","slug":"shadow-removal","permalink":"https://baidut.github.io/tags/shadow-removal/"}]},{"title":"Backpropagation","slug":"backpropagation","date":"2017-08-04T06:56:27.000Z","updated":"2017-09-05T15:18:13.073Z","comments":true,"path":"2017/08/04/backpropagation/","link":"","permalink":"https://baidut.github.io/2017/08/04/backpropagation/","excerpt":"","text":"还是先画关系图 解法一：局部到全体 解法二：反向递推 神经元间到神经元内：由于网络结构以神经元为单位，以 z 代表一个神经元求，先省略 w,b 神经元间解法一：局部到全体 (分子布局)先求解各个边，再找路径，路径上相乘，路径间相加。 输出层到损失函数$$\\frac{ \\partial C}{\\partial z_i^{(L)}} = \\frac{ \\partial C}{\\partial a_i^{(L)}} \\frac{\\partial a_i^{(L)}}{ \\partial z_i^{(L)}}= \\frac{ \\partial C}{\\partial a_i^{(L)}} f’({ z_i^{(L)}})$$ 分子布局的矩阵形式为(只有 a_i 和 z_i 有关，所以为对角阵)：$$\\frac{ \\partial C}{\\partial \\mathbf z^{(L)}} = \\frac{ \\partial C}{\\partial \\mathbf a^{(L)}} \\frac{\\partial \\mathbf a^{(L)}}{ \\partial \\mathbf z^{(L)}}= \\frac{ \\partial C}{\\partial \\mathbf a^{(L)}} diag(f’(\\mathbf z^{(L)}))$$Hadamard 乘积形式为 (混合布局，维持主体)：$$\\frac{ \\partial C}{\\partial \\mathbf z^{(L)}} = C’(\\mathbf a^{(L)}) \\odot f’(\\mathbf z^{(L)})$$ 相邻层$$\\frac{ \\partial z^{(l+1)}_j}{\\partial z_i^{(l)}} = \\frac{ \\partial z^{(l+1)}_j}{\\partial a_i^{(l)}} \\frac{\\partial a_i^{(l)}}{ \\partial zi^{(l)}}= w^{(l+1)}{j←i} f’({ z_i^{(l)}})$$ 矩阵形式：$w^{(l+1)}{j←i}$ 合并成 $\\mathbf W^{(l+1)} $ 矩阵的元素，l+1行，l列，矩阵每一行都是一个权值向量，$w^{(l+1)}{j←i} = w^{(l+1)}_{ji} $$$\\frac{ \\partial \\mathbf z^{(l+1)}}{\\partial \\mathbf z^{(l)}} = \\frac{ \\partial \\mathbf z^{(l+1)}}{\\partial \\mathbf a^{(l)}} \\frac{\\partial \\mathbf a^{(l)}}{ \\partial \\mathbf z^{(l)}}= \\mathbf W^{(l+1)} diag(f’(\\mathbf z^{(l)}))$$无法写成 Hadamard 乘积形式 路径$$\\frac{ \\partial C}{\\partial zi^{(l)}} = \\sum{mnk … pq} \\frac{ \\partial C}{\\partial z_m^{(L)}} \\frac{\\partial z_m^{(L)}}{ \\partial z_n^{(L-1)}} \\frac{\\partial z_n^{(L-1)}}{ \\partial z_k^{(L-2)}} … \\frac{\\partial z_p^{(l+1)}}{ \\partial z_q^{(l)}}$$ 不好计算，直接看矩阵形式：$$\\begin{align}\\frac{ \\partial C}{\\partial \\mathbf z^{(l)}}=&amp; \\frac{ \\partial C}{\\partial \\mathbf z^{(L)}} \\frac{\\partial \\mathbf z^{(L)}}{ \\partial \\mathbf z^{(L-1)}} \\frac{\\partial \\mathbf z^{(L-1)}}{ \\partial \\mathbf z^{(L-2)}} … \\frac{\\partial \\mathbf z^{(l+1)}}{ \\partial \\mathbf z^{(l)}}\\=&amp;\\frac{ \\partial C}{\\partial \\mathbf a^{(L)}} diag(f’(\\mathbf z^{(L)}))\\mathbf W^{(L)} diag(f’(\\mathbf z^{(L-1)}))\\mathbf W^{(L-1)} diag(f’(\\mathbf z^{(L-2)}))… \\&amp; \\mathbf W^{(l+1)} diag(f’(\\mathbf z^{(l)}))\\end{align}$$Hadamard 乘积形式：$$\\begin{align}\\frac{ \\partial C}{\\partial \\mathbf z^{(l)}}=&amp; \\frac{ \\partial C}{\\partial \\mathbf z^{(L)}} \\frac{\\partial \\mathbf z^{(L)}}{ \\partial \\mathbf z^{(L-1)}} \\frac{\\partial \\mathbf z^{(L-1)}}{ \\partial \\mathbf z^{(L-2)}} … \\frac{\\partial \\mathbf z^{(l+1)}}{ \\partial \\mathbf z^{(l)}}\\=&amp; C’(\\mathbf a^{(L)}) \\odot f’(\\mathbf z^{(L)})\\mathbf W^{(L)} \\odot f’(\\mathbf z^{(L-1)})\\mathbf W^{(L-1)} \\odot f’(\\mathbf z^{(L-2)})… \\&amp; \\mathbf W^{(l+1)} \\odot f’(\\mathbf z^{(l)})\\end{align}$$递推关系:$$\\begin{align}\\frac{ \\partial C}{\\partial \\mathbf z^{(l)}}=&amp; \\frac{ \\partial C}{\\partial \\mathbf z^{(l+1)}} \\frac{\\partial \\mathbf z^{(l+1)}}{ \\partial \\mathbf z^{(l)}}\\=&amp;\\frac{ \\partial C}{\\partial \\mathbf z^{(l)}} \\mathbf W^{(l+1)} diag(f’(\\mathbf z^{(l)}))\\end{align}$$ 解法二：反向递推 (分母布局)定义$$\\delta_i^{(l)} = \\frac{ \\partial C}{\\partial z_i^{(l)}}$$列向量$$\\boldsymbol \\delta^{(l)} = \\frac{ \\partial C}{\\partial \\mathbf z^{(l)}}$$ 递推首项 $\\delta_i^{(L)} $$$\\delta_i^{(L)} = \\frac{\\partial a_i^{(L)}}{ \\partial z_i^{(L)}} \\frac{ \\partial C}{\\partial a_i^{(L)}}= f’({ z_i^{(L)}}) \\frac{ \\partial C}{\\partial a_i^{(L)}}$$ 矩阵形式$$\\boldsymbol \\delta^{(L)} = \\frac{\\partial \\mathbf a^{(L)}}{ \\partial \\mathbf z^{(L)}} \\frac{ \\partial C}{\\partial \\mathbf a^{(L)}}= diag(f’(\\mathbf z^{(L)})) \\frac{ \\partial C}{\\partial \\mathbf a^{(L)}}$$Hadamard 乘积形式：$$\\boldsymbol \\delta^{(L)} = f’(\\mathbf z^{(L)}) \\odot C’(\\mathbf a^{(L)})$$ 递推公式 $\\delta_i^{(l)}$与 $\\delta_i^{(l+1)} $$$\\delta_i^{(l)}= \\frac{ \\partial C}{\\partial z_i^{(l)}} = \\sum_j \\frac{ \\partial z_j^{(l+1)}}{\\partial z_i^{(l)}} \\frac{ \\partial C}{\\partial z_j^{(l+1)}}= \\sum_j \\frac{ \\partial z_j^{(l+1)}}{\\partial z_i^{(l)}} \\delta_j^{(l+1)}$$ $$\\delta_i^{(l)} = \\sum_j f’({ zi^{(l)}}) w^{(l+1)}{j←i} \\delta_j^{(l+1)}$$矩阵形式：$$\\boldsymbol \\delta^{(l)} =diag(f’(\\mathbf z^{(L)})) (\\mathbf W^{(l+1)})^T \\boldsymbol \\delta^{(l+1)}$$展开后和前面的结果只差一个转置的关系。 Hadamard 乘积形式：$$\\boldsymbol \\delta^{(l)} = f’({ \\mathbf z^{(l)}}) \\odot ((\\mathbf W^{(l+1)} )^T \\boldsymbol \\delta^{(l+1)} )$$可以看出递推的写法更简洁。 神经元内下面看 w 和 b$$\\begin{align}a^{(l)}_i &amp;= f ( z^{(l)}_i ) \\z^{(l)}i &amp;= \\sum{j} w^{(l)}_{ i ←j} \\times a^{(l-1)}_j + b^{(l)}i\\end{align}$$偏导：$$\\begin{align}\\frac{ \\partial C}{\\partial w{ij}^{(l)}} &amp;= \\delta_i^{(L)} {a_j}^{(l-1)}\\\\frac{ \\partial C}{\\partial b_i^{(l)}} &amp; = \\delta_i^{(L)}\\end{align}$$ 矩阵形式 $\\mathbf{w}^{(l)}i :1 \\times N{l-1}$，$\\mathbf{W}^{(l)}:Nl \\times N{l-1}$：$$z^{(l)}_i = \\mathbf{w}^{(l)}_i \\mathbf{a}^{(l-1)} + {b}^{(l)}_i, \\\\mathbf{z}^{(l)} = \\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)},$$偏导(分子布局)：$$\\begin{align}\\frac{ \\partial \\mathbf z^{(l)}}{\\partial \\mathbf a^{(l-1)}} &amp; = \\mathbf{W}^{(l)}\\\\frac{ \\partial \\mathbf z^{(l)}}{\\partial \\mathbf W^{(l)}} &amp; = (\\mathbf{a}^{(l-1)})^T\\\\frac{ \\partial \\mathbf z^{(l)}}{\\partial \\mathbf b^{(l)}} &amp; = \\mathbf I^{(l\\times l)}\\\\end{align}$$结论 递推式：$$\\begin{align}\\frac{ \\partial C}{\\partial \\mathbf W^{(l)}} &amp; = \\boldsymbol \\delta^{(l)} (\\mathbf{a}^{(l-1)})^T\\\\frac{ \\partial C}{\\partial \\mathbf b^{(l)}} &amp; = \\boldsymbol \\delta^{(l)}\\end{align}$$ 梯度消失和梯度爆炸下一讲，程序实现。","categories":[{"name":"lecture","slug":"lecture","permalink":"https://baidut.github.io/categories/lecture/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"https://baidut.github.io/tags/deep-learning/"}]},{"title":"ORGB：Offset correction in RGB Color Space for Illumination-Robust Image Processing","slug":"icassp2017orgb","date":"2017-03-04T16:00:00.000Z","updated":"2017-09-04T13:20:10.068Z","comments":true,"path":"2017/03/05/icassp2017orgb/","link":"","permalink":"https://baidut.github.io/2017/03/05/icassp2017orgb/","excerpt":"Single materials have colors which form straight lines in RGB space. However, in severe shadow cases, those lines do not intersect the origin, which is inconsistent with the description of most literature. This paper is concerned with the detection and correction of the offset between the intersection and origin. First, we analyze the reason for forming that offset via an optical imaging model. Second, we present a simple and effective way to detect and remove the offset. The resulting images, named ORGB, have almost the same appearance as the original RGB images while are more illumination-robust for color space conversion. Besides, image processing using ORGB instead of RGB is free from the interference of shadows. Finally, the proposed offset correction method is ap- plied to road detection task, improving the performance both in quantitative and qualitative evaluations.","text":"Single materials have colors which form straight lines in RGB space. However, in severe shadow cases, those lines do not intersect the origin, which is inconsistent with the description of most literature. This paper is concerned with the detection and correction of the offset between the intersection and origin. First, we analyze the reason for forming that offset via an optical imaging model. Second, we present a simple and effective way to detect and remove the offset. The resulting images, named ORGB, have almost the same appearance as the original RGB images while are more illumination-robust for color space conversion. Besides, image processing using ORGB instead of RGB is free from the interference of shadows. Finally, the proposed offset correction method is ap- plied to road detection task, improving the performance both in quantitative and qualitative evaluations. 12345678@inproceedings&#123;ying2017orgb, title=&#123;ORGB: Offset correction in RGB color space for illumination-robust image processing&#125;, author=&#123;Ying, Zhenqiang and Li, Ge and Wen, Sixin and Tan, Guozhen&#125;, booktitle=&#123;Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on&#125;, pages=&#123;1557--1561&#125;, year=&#123;2017&#125;, organization=&#123;IEEE&#125;&#125; project website arxiv","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"shadow removal","slug":"shadow-removal","permalink":"https://baidut.github.io/tags/shadow-removal/"},{"name":"image analysis","slug":"image-analysis","permalink":"https://baidut.github.io/tags/image-analysis/"}]},{"title":"Searching Action Proposals via Spatial Actionness Estimation and Temporal Path Inference and Tracking","slug":"accv2016","date":"2016-11-19T16:00:00.000Z","updated":"2017-09-05T09:43:21.307Z","comments":true,"path":"2016/11/20/accv2016/","link":"","permalink":"https://baidut.github.io/2016/11/20/accv2016/","excerpt":"In this paper, we address the problem of searching action proposals in unconstrained video clips. Our approach starts from actionness estimation on frame-level bounding boxes, and then aggregates the bounding boxes belonging to the same actor across frames via linking, associating, tracking to generate spatial-temporal continuous action paths. To achieve the target, a novel actionness estimation method is firstly proposed by utilizing both human appearance and motion cues. Then, the association of the action paths is formulated as a maximum set coverage problem with the results of actionness estimation as a priori as in [1]. To further promote the performance, we design an improved optimization objective for the problem and provide a greedy search algorithm to solve it. Finally, a tracking-by-detection scheme is designed to further refine the searched action paths. Extensive experiments on two challenging datasets, UCF-Sports and UCF-101, show that the proposed approach advances state-of-the-art proposal generation performance in terms of both accuracy and proposal quantity.","text":"In this paper, we address the problem of searching action proposals in unconstrained video clips. Our approach starts from actionness estimation on frame-level bounding boxes, and then aggregates the bounding boxes belonging to the same actor across frames via linking, associating, tracking to generate spatial-temporal continuous action paths. To achieve the target, a novel actionness estimation method is firstly proposed by utilizing both human appearance and motion cues. Then, the association of the action paths is formulated as a maximum set coverage problem with the results of actionness estimation as a priori as in [1]. To further promote the performance, we design an improved optimization objective for the problem and provide a greedy search algorithm to solve it. Finally, a tracking-by-detection scheme is designed to further refine the searched action paths. Extensive experiments on two challenging datasets, UCF-Sports and UCF-101, show that the proposed approach advances state-of-the-art proposal generation performance in terms of both accuracy and proposal quantity. 12345678@inproceedings&#123;li2016searching, title=&#123;Searching Action Proposals via Spatial Actionness Estimation and Temporal Path Inference and Tracking&#125;, author=&#123;Li, Nannan and Xu, Dan and Ying, Zhenqiang and Li, Zhihao and Li, Ge&#125;, booktitle=&#123;Asian Conference on Computer Vision&#125;, pages=&#123;384--399&#125;, year=&#123;2016&#125;, organization=&#123;Springer&#125;&#125; arXiv The framework of our action proposal generation approach","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"action recognition","slug":"action-recognition","permalink":"https://baidut.github.io/tags/action-recognition/"},{"name":"machine learning","slug":"machine-learning","permalink":"https://baidut.github.io/tags/machine-learning/"}]},{"title":"A Novel Shadow-Free Feature Extractor for Real-Time Road Detection","slug":"mm2016","date":"2016-09-30T16:00:00.000Z","updated":"2017-09-05T09:47:22.970Z","comments":true,"path":"2016/10/01/mm2016/","link":"","permalink":"https://baidut.github.io/2016/10/01/mm2016/","excerpt":"Road detection is one of the most important research areas in driver assistance and automated driving field. However, the performance of existing methods is still unsatisfactory, especially in severe shadow conditions. To overcome those difficulties, first we propose a novel shadow-free feature extractor based on the color distribution of road surface pixels. Then we present a road detection framework based on the extractor, whose performance is more accurate and robust than that of existing extractors. Also, the proposed framework has much low-complexity, which is suitable for usage in practical systems.","text":"Road detection is one of the most important research areas in driver assistance and automated driving field. However, the performance of existing methods is still unsatisfactory, especially in severe shadow conditions. To overcome those difficulties, first we propose a novel shadow-free feature extractor based on the color distribution of road surface pixels. Then we present a road detection framework based on the extractor, whose performance is more accurate and robust than that of existing extractors. Also, the proposed framework has much low-complexity, which is suitable for usage in practical systems. 12345678@inproceedings&#123;ying2016novel, title=&#123;A Novel Shadow-Free Feature Extractor for Real-Time Road Detection&#125;, author=&#123;Ying, Zhenqiang and Li, Ge and Zang, Xianghao and Wang, Ronggang and Wang, Wenmin&#125;, booktitle=&#123;Proceedings of the 2016 ACM on Multimedia Conference&#125;, pages=&#123;611--615&#125;, year=&#123;2016&#125;, organization=&#123;ACM&#125;&#125; ACM Digital Library ResearchGate","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"shadow removal","slug":"shadow-removal","permalink":"https://baidut.github.io/tags/shadow-removal/"},{"name":"image analysis","slug":"image-analysis","permalink":"https://baidut.github.io/tags/image-analysis/"}]},{"title":"Robust Lane Marking Detection using Boundary-Based Inverse Perspective Mapping","slug":"icassp2016bird","date":"2016-03-19T16:00:00.000Z","updated":"2017-09-04T13:44:25.479Z","comments":true,"path":"2016/03/20/icassp2016bird/","link":"","permalink":"https://baidut.github.io/2016/03/20/icassp2016bird/","excerpt":"Road detection, which brings a visual perceptive ability to vehicles, is essential to build driver assistance systems. To help detect lane markings in challenging scenarios, one-time calibration of inverse perspective mapping (IPM) parameters is employed to build a bird’s eye view of the road image. We propose an automatic IPM method based on road boundaries called BIRD (Boundary-based IPM for Road Detection), avoiding common problems of fixed IPM. Furthermore, integrating top-down and bottom-up attention, an illumination-robust lane marking detection approach using BIRD is proposed.","text":"Road detection, which brings a visual perceptive ability to vehicles, is essential to build driver assistance systems. To help detect lane markings in challenging scenarios, one-time calibration of inverse perspective mapping (IPM) parameters is employed to build a bird’s eye view of the road image. We propose an automatic IPM method based on road boundaries called BIRD (Boundary-based IPM for Road Detection), avoiding common problems of fixed IPM. Furthermore, integrating top-down and bottom-up attention, an illumination-robust lane marking detection approach using BIRD is proposed. 12345678@inproceedings&#123;ying2016robust, title=&#123;Robust lane marking detection using boundary-based inverse perspective mapping&#125;, author=&#123;Ying, Zhenqiang and Li, Ge&#125;, booktitle=&#123;Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on&#125;, pages=&#123;1921--1925&#125;, year=&#123;2016&#125;, organization=&#123;IEEE&#125;&#125;","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"shadow removal","slug":"shadow-removal","permalink":"https://baidut.github.io/tags/shadow-removal/"},{"name":"image analysis","slug":"image-analysis","permalink":"https://baidut.github.io/tags/image-analysis/"},{"name":"road detection","slug":"road-detection","permalink":"https://baidut.github.io/tags/road-detection/"}]},{"title":"An Illumination-Robust Approach for Feature-Based Road Detection","slug":"ism2015","date":"2015-12-13T16:00:00.000Z","updated":"2017-09-04T13:46:47.007Z","comments":true,"path":"2015/12/14/ism2015/","link":"","permalink":"https://baidut.github.io/2015/12/14/ism2015/","excerpt":"Road detection algorithms constitute a basis for intelligent vehicle systems which are designed to improve safety and efficiency for human drivers. In this paper, a novel road detection approach intended for tackling illumination-related effects is proposed. First, a grayscale image of modified saturation is derived from the input color image during preprocessing, effectively diminishing cast shadows. Second, the road boundary lines are detected, which provides an adaptive region of interest for the following lane-marking detection. Finally, an improved feature-based method is employed to identify lane-markings from the shadows. The experimental results show that the proposed approach is robust against illumination-related effects..","text":"Road detection algorithms constitute a basis for intelligent vehicle systems which are designed to improve safety and efficiency for human drivers. In this paper, a novel road detection approach intended for tackling illumination-related effects is proposed. First, a grayscale image of modified saturation is derived from the input color image during preprocessing, effectively diminishing cast shadows. Second, the road boundary lines are detected, which provides an adaptive region of interest for the following lane-marking detection. Finally, an improved feature-based method is employed to identify lane-markings from the shadows. The experimental results show that the proposed approach is robust against illumination-related effects.. 12345678@inproceedings&#123;ying2015illumination, title=&#123;An illumination-robust approach for feature-based road detection&#125;, author=&#123;Ying, Zhenqiang and Li, Ge and Tan, Guozhen&#125;, booktitle=&#123;Multimedia (ISM), 2015 IEEE International Symposium on&#125;, pages=&#123;278--281&#125;, year=&#123;2015&#125;, organization=&#123;IEEE&#125;&#125;","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"image analysis","slug":"image-analysis","permalink":"https://baidut.github.io/tags/image-analysis/"},{"name":"road detection","slug":"road-detection","permalink":"https://baidut.github.io/tags/road-detection/"}]},{"title":"JPEGHexViewer","slug":"JPEGParser","date":"2015-01-30T16:00:00.000Z","updated":"2017-09-04T14:53:01.038Z","comments":true,"path":"2015/01/31/JPEGParser/","link":"","permalink":"https://baidut.github.io/2015/01/31/JPEGParser/","excerpt":"","text":"The JPEGHexViewer is a tool to help you figure out the syntax structure of JPEG file. It is written in C++, using Qt (Qt5) framework. Source Code","categories":[{"name":"Projects","slug":"Projects","permalink":"https://baidut.github.io/categories/Projects/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://baidut.github.io/tags/C/"}]},{"title":"Build a six-legged robot (RHEX)","slug":"rhexapod","date":"2014-12-05T16:00:00.000Z","updated":"2017-09-05T07:35:07.282Z","comments":true,"path":"2014/12/06/rhexapod/","link":"","permalink":"https://baidut.github.io/2014/12/06/rhexapod/","excerpt":"","text":"We built an RHEX (a six-legged robot inspired by hexapod cockroach). Let’s watch a video to know this interesting robot! I put some notes taken when building our robot here just for helping me rewind the memory. Related Posts: Hello RHEX Hello Webots Simulate the alternate tripod gait using Webots","categories":[{"name":"Projects","slug":"Projects","permalink":"https://baidut.github.io/categories/Projects/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://baidut.github.io/tags/C/"}]},{"title":"Simulate the alternate tripod gait using Webots","slug":"alternate-tripod-gait","date":"2014-12-05T16:00:00.000Z","updated":"2017-09-04T15:36:30.306Z","comments":true,"path":"2014/12/06/alternate-tripod-gait/","link":"","permalink":"https://baidut.github.io/2014/12/06/alternate-tripod-gait/","excerpt":"","text":"本文介绍三脚架对换式步态 （alternate tripod gait）的实现 1234┌───────────┐ ^ right│ 0 1 2 │ │ │ 3 4 5 │ front &lt;──┘└───────────┘ 一侧的前后两足和另一侧的中间足构成一个三脚架，每一时刻有一个三脚架处于支撑相，另一个三脚架出于摆动相，每隔半个周期进行切换。 足沿着轴摆动时，我们把处于支撑相的这段叫做小摆(smallSwap)，把处于摆动相的这段叫做大摆(bigSwap)。 123#define RATE 1.0/6 // 小摆占圆周的比率#define smallSwap 2 * M_PI * RATE#define bigSwap 2 * M_PI * (1-RATE) 步态涉及两个状态，一个是奇数号三脚架大摆，偶数号三脚架小摆；另一个是奇数号三脚架小摆，偶数号三脚架大摆。 1234const double posSwap [NUM_STATES][NUM_SERVOS] = &#123; &#123;smallSwap ,bigSwap ,smallSwap ,bigSwap ,smallSwap ,bigSwap &#125;, &#123;bigSwap ,smallSwap ,bigSwap ,smallSwap ,bigSwap ,smallSwap&#125;&#125;; 初始时六足偶数号在前（进入支撑相），奇数号在后（离开地面进入摆动相）。准备进入第一个状态。 1234double pos[NUM_SERVOS] = &#123; -smallSwap/2,+smallSwap/2,-smallSwap/2, +smallSwap/2,-smallSwap/2,+smallSwap/2&#125;; 主循环每隔一定周期调整一次各个足的位置，进入下一个状态。 1234567891011while(1) &#123;#define AJUST_PERIOD 50 elapsed++; if(elapsed % AJUST_PERIOD!=0) continue; state = !state; for (i = 0; i &lt; NUM_SERVOS; i++ )&#123; servo_set_position(servos[i], pos[i]); pos[i] += posSwap[state][i]; &#125;&#125; 调整两侧足的运行方向，两侧足同向后运行，就可以实现倒退；一侧足向前，一侧足向后，就可以实现原地转弯。 1234const int goForward [] = &#123; 1, 1, 1, 1, 1, 1&#125;;const int goBackward [] = &#123;-1,-1,-1,-1,-1,-1&#125;;const int turnLeft [] = &#123; 1, 1, 1,-1,-1,-1&#125;;const int turnRight [] = &#123;-1,-1,-1, 1, 1, 1&#125;; 加上转向，完整的仿真程序如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#include &lt;webots/robot.h&gt;#include &lt;webots/servo.h&gt;#include &lt;stdio.h&gt;#define NUM_SERVOS 6#define NUM_STATES 2#define RATE 1.0/6#define smallSwap 2 * M_PI * RATE#define bigSwap 2 * M_PI * (1-RATE)#define TIME_STEP 16#define AJUST_PERIOD 50int main() &#123; const char *SERVO_NAMES[] = &#123; \"servo_r0\", \"servo_r1\", \"servo_r2\", \"servo_l0\", \"servo_l1\", \"servo_l2\" &#125;; WbDeviceTag servos[NUM_SERVOS]; const int goForward [] = &#123; 1, 1, 1, 1, 1, 1&#125;; const int goBackward [] = &#123;-1,-1,-1,-1,-1,-1&#125;; const int turnLeft [] = &#123; 1, 1, 1,-1,-1,-1&#125;; const int turnRight [] = &#123;-1,-1,-1, 1, 1, 1&#125;; const double posSwap [NUM_STATES][NUM_SERVOS] = &#123; &#123;smallSwap ,bigSwap ,smallSwap ,bigSwap ,smallSwap ,bigSwap &#125;, &#123;bigSwap ,smallSwap ,bigSwap ,smallSwap ,bigSwap ,smallSwap&#125; &#125;; double pos[NUM_SERVOS] = &#123; -smallSwap/2,+smallSwap/2,-smallSwap/2, +smallSwap/2,-smallSwap/2,+smallSwap/2 &#125;; int i; // 初始化Webots链接舵机 wb_robot_init(); for (i = 0; i &lt; NUM_SERVOS; i++) &#123; servos[i] = wb_robot_get_device(SERVO_NAMES[i]); if (!servos[i]) printf(\"could not find servo: %s\\n\",SERVO_NAMES[i]); &#125; const int* dir = turnRight;//turnLeft;//goBackward;// goForward; int state; int elapsed = 0; // 初始位置 for (i = 0; i &lt; NUM_SERVOS; i++ ) pos[i] = dir[i] * pos[i]; // 主循环 while(wb_robot_step(TIME_STEP)!=-1) &#123; elapsed++; if(elapsed % AJUST_PERIOD!=0) continue; state = (elapsed / AJUST_PERIOD + 1) % NUM_STATES; for (i = 0; i &lt; NUM_SERVOS; i++ )&#123; wb_servo_set_position(servos[i], pos[i]); pos[i] += dir[i] * posSwap[state][i]; &#125; &#125; wb_robot_cleanup(); return 0;&#125; 仿真结果如下：","categories":[{"name":"Programming","slug":"Programming","permalink":"https://baidut.github.io/categories/Programming/"}],"tags":[{"name":"robot","slug":"robot","permalink":"https://baidut.github.io/tags/robot/"},{"name":"C","slug":"C","permalink":"https://baidut.github.io/tags/C/"}]},{"title":"Hello Webots","slug":"hello-webots","date":"2014-08-25T16:00:00.000Z","updated":"2017-09-04T15:36:40.360Z","comments":true,"path":"2014/08/26/hello-webots/","link":"","permalink":"https://baidut.github.io/2014/08/26/hello-webots/","excerpt":"","text":"构建世界 img 在视图菜单下的 最后一项 显示灯光位置 这点和说明书不匹配 img 改变外观属性 img 添加一个放在地面上的物体，如果添加物理属性需要设置边界模型，否则就掉下去了。。。 img 按照例程添加墙 img 添加机器人，差动轮模型加上仅用于显示的外观 img 处理碰撞时其实是以一个box来处理，而不是按照曲面处理的 摆弄一个下午Webots的成果，整理一下。 编程控制空的模板 void.c 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/* * File: * Date: * Description: * Author: * Modifications: *//* * You may need to add include files like &lt;webots/distance_sensor.h&gt; or * &lt;webots/differential_wheels.h&gt;, etc. */#include &lt;webots/robot.h&gt;/* * You may want to add defines macro here. */#define TIME_STEP 64/* * You should put some helper functions here *//* * This is the main program. * The arguments of the main function can be specified by the * \"controllerArgs\" field of the Robot node */int main(int argc, char **argv)&#123; /* necessary to initialize webots stuff */ wb_robot_init(); /* * You should declare here DeviceTag variables for storing * robot devices like this: * WbDeviceTag my_sensor = wb_robot_get_device(\"my_sensor\"); * WbDeviceTag my_actuator = wb_robot_get_device(\"my_actuator\"); */ /* main loop */ do &#123; /* * Read the sensors : * Enter here functions to read sensor data, like: * double val = wb_distance_sensor_get_value(my_sensor); */ /* Process sensor data here */ /* * Enter here functions to send actuator commands, like: * wb_differential_wheels_set_speed(100.0,100.0); */ /* * Perform a simulation step of 64 milliseconds * and leave the loop when the simulation is over */ &#125; while (wb_robot_step(TIME_STEP) != -1); /* Enter here exit cleanup code */ /* Necessary to cleanup webots stuff */ wb_robot_cleanup(); return 0;&#125; 直接修改例程 hexapod.c hexapod例程源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/* * File: hexapod.c * Date: September 22th, 2005 * Description: Alternate tripod gait using linear servos * Author: Yvan Bourquin * Modifications:Simon Blanchoud - September 12th, 2006 * Indentation of the code so that it follows the Webots Coding * Standards * * Copyright (c) 2006 Cyberbotics - www.cyberbotics.com */#include &lt;webots/robot.h&gt;#include &lt;webots/servo.h&gt;#include &lt;stdio.h&gt;#define TIME_STEP 16#define NUM_SERVOS 12#define NUM_STATES 6#define FRONT +0.7#define BACK -0.7#define HI +0.02#define LO -0.02int main() &#123; const char *SERVO_NAMES[NUM_SERVOS] = &#123; \"hip_servo_r0\", \"hip_servo_r1\", \"hip_servo_r2\", \"hip_servo_l0\", \"hip_servo_l1\", \"hip_servo_l2\", \"knee_servo_r0\", \"knee_servo_r1\", \"knee_servo_r2\", \"knee_servo_l0\", \"knee_servo_l1\", \"knee_servo_l2\"&#125;; WbDeviceTag servos[NUM_SERVOS]; const double pos[NUM_STATES][NUM_SERVOS] = &#123; &#123;BACK, FRONT, BACK, -FRONT, -BACK, -FRONT, LO, HI, LO, HI, LO, HI&#125;, &#123;BACK, FRONT, BACK, -FRONT, -BACK, -FRONT, HI, HI, HI, HI, HI, HI&#125;, &#123;BACK, FRONT, BACK, -FRONT, -BACK, -FRONT, HI, LO, HI, LO, HI, LO&#125;, &#123;FRONT, BACK, FRONT, -BACK, -FRONT, -BACK, HI, LO, HI, LO, HI, LO&#125;, &#123;FRONT, BACK, FRONT, -BACK, -FRONT, -BACK, HI, HI, HI, HI, HI, HI&#125;, &#123;FRONT, BACK, FRONT, -BACK, -FRONT, -BACK, LO, HI, LO, HI, LO, HI&#125; &#125;; int elapsed = 0; int state, i; wb_robot_init(); for (i = 0; i &lt; NUM_SERVOS; i++) &#123; servos[i] = wb_robot_get_device(SERVO_NAMES[i]); if (!servos[i]) &#123; printf(\"could not find servo: %s\\n\",SERVO_NAMES[i]); &#125; &#125; while(wb_robot_step(TIME_STEP)!=-1) &#123; elapsed++; state = (elapsed / 25 + 1) % NUM_STATES; for (i = 0; i &lt; NUM_SERVOS; i++) &#123; wb_servo_set_position(servos[i], pos[state][i]); &#125; &#125; wb_robot_cleanup(); return 0;&#125; img 修改电机旋转方向 img 去掉步态数组的负号 原地转 img 受设定的舵机范围限制，只能转到极限角度 img 控制周期内来不及调整的话，就会一直打转 img 改成电机的 原地打转 不需要伸缩足了 先在例子上直接修改，了解一下后续流程。将hexapod例程中的舵机的旋转方向修改一下，就可以行走了！ 注意没有伸缩腿是走不动的，前后力量抵消，原地运动。 对步态数组进行简单修改试试，去掉里面的负号，得到原地打转的效果。 下面把舵机换成电机，需要取消对打角最大最小值的限定。 查阅了一下手册， When both minPosition and maxPosition are zero (the default), the soft limits are deactivated 都设置为0的时候就没有限制了！好了，下面可以试试电机的效果了，注意转角的范围是负无穷到正无穷。控制的话就是每次增加一定的转角值就行了。注意还要调整一下控制周期，如果在控制周期内来不及调整的话，就会一直打转（都以最大力量向前摆动）。 servo作为电机使用的旋转写法： 12345678910const char *SERVO_NAME = \"servo\";WbDeviceTag servo;servo = wb_robot_get_device(SERVO_NAME);if (!servo) &#123; printf(\"could not find servo: %s\\n\",servo);&#125;wb_servo_set_position(servo, INFINITY);wb_servo_set_velocity(servo, 6.28); // 1 rotation per second 接触面 img 狗与地面接触的部分使用圆球模拟的 img 一个接触面 关于构造自定义边界物体的问题 边界物体是仿真的重点，外观做多么华丽都对物理仿真没什么区别，只是看着不同罢了。 不是所有的形状都可以作为边界物体的： 采用USE使用一个形状时，出现问题： DEF LEG Robot (name=’solid’): boundingObject: ignoring illegal Extrusion node in the geometry field of a Shape node Failed to create geometry for a Extrusion object 测试了一下默认的Extrusion形状能否构建边界物体，结果失败了， 还是上面的提示，于是找官网说明，在官网说明书第七节中，介绍了如何选择边界对象的问题，引用原文如下： 7.6.5 How to choose bounding Objects? As said before, minimizing the number of bounding objects increases the simulation speed. However choosing the bounding objects primitives carefully is also crucial to increase the simulation speed. Using a combination of Sphere, Box, Capsule and Cylinder nodes for defining objects is very efficient. Generally speaking, the efficiency of these primitives can be sorted like this: Sphere &gt; Box &gt; Capsule &gt; Cylinder. Where the Sphere is the most efficient. But this can be neglected for a common usage. The IndexdedFaceSet geometry primitive can also be used in a bounding object. But this primitive is less efficientthan the other primitives listed above. Moreover its behavior is sometimes buggy. For this reasons, we don’t recommend using the IndexdedFaceSet when another solution using a combination of the other primitives is possible. Grounds can be defined using the Plane or the ElevationGrid primitives. The Plane node is much more efficient than the ElevationGrid node, but it can only be used to model a flat terrain while the ElevationGrid can be used to model an uneven terrain. 因此选择球形最利于仿真，从软件自带手册的关于形状的一节可以看出，对于特殊的形状，要么用支持的简单形状组合，要么可以用 IndexdedFaceSet 绘制。 而IndexdedFaceSet 的绘制是非常麻烦的，大概需要通过每一个构成面的各个顶点以及顶点索引序列声明各个构成面，还要通过构成边索引和边的二维坐标值声明构成边，简直要吐血。而且还有问题： More than 3 vertices defined for a face in IndexedFaceSet, can’t create boundingObject 一张面必须是3个点构成的，否则不能构造碰撞物体。 再加上前面说明的低效率less efficient 和可能出错sometimes buggy，还是选择组合的方式吧。 机器人构建 img img robot结构树 img 让我们从前面的机器人身上碾压过去 哈哈 img 步态慢动作 先创建一个身体，solid完成可以转换为robot（闪电工具） 添加上电机，场景结构树设计可参考狗机器人的结构树。 创建好了后输出（场景树上方按钮），就可以带上机器人到各种世界逛逛了。。。","categories":[{"name":"Programming","slug":"Programming","permalink":"https://baidut.github.io/categories/Programming/"}],"tags":[{"name":"robot","slug":"robot","permalink":"https://baidut.github.io/tags/robot/"},{"name":"C","slug":"C","permalink":"https://baidut.github.io/tags/C/"}]},{"title":"Hello RHEX","slug":"hello-rhex","date":"2014-08-25T16:00:00.000Z","updated":"2017-09-04T15:39:50.542Z","comments":true,"path":"2014/08/26/hello-rhex/","link":"","permalink":"https://baidut.github.io/2014/08/26/hello-rhex/","excerpt":"","text":"足式机器人和轮式机器人的结合-旋转足式机器人 足式机器人能够通过移动足灵活地调整行走姿态，具有较强的地形适应能力，但往往速度慢，负载少；轮式机器人解决了以上问题，但往往只能应用于平地运输。RHex是一种旋转足式的机器人，兼得两者的优点，不仅能够应对复杂地形，还具备负载能力和非常快的速度，还可以跳跃。 跳跃能力 大多数陆地动物都拥有跳跃能力，这是一个非常重要的生存技能，如果没有了跳跃，那估计有很多事情都做不了。而对于机器人而言，没有跳跃能力的变得更为显眼，因此机器人的自由程度非常有限，遇到一些沟槽和矮墙，就直接瘫痪了。 在阿富汗战场上，波士顿动力公司的Sand Flea机器人凭借着出色的可移动性为美国大兵打来了极大的帮助，而这也带动了跳跃机器人的发展。就在这款机器人闻名的几个月后，美国国防部高级研究计划局就开始研制RHex，它放弃纯粹的竖直跳跃以支持多种多样的移动方式，不仅可以跳跃，还能游泳、爬楼梯，可移动性更强。 RHex共具有6条腿，被设计成弯曲的弧状，这种巧妙的设计能允许机器人做出很多匪夷所思的跳跃。RHex可以双足跳跃，四足跳跃、六足跳跃，还可以连续跳跃。通过不同的跳跃模式达到不同的效果，比如跳跃沟槽，攀爬矮墙，或者是180跳跃翻身。它能以抛物线的形状跳过很长的鸿沟；可以自动根据之前的跳跃，确定下一步跳跃的范围。 水陆两栖 RHex不仅能跳能爬，由于其粗糙的外表设计，它还是防水的，这意味着他能完全潜到水底。而且，它弧状的腿也很适合游泳。通过精准控制这6条腿，这款机器人能游过很长距离的海滩，而且速度可以达到10千米每小时。另外，它还能攀爬螺旋式的阶梯，所以一旦它被运用到战场上，将比Sand Flea发挥更大的作用。 结构特征行走机构分别为六个相同大小的偏心圆，均匀分布在机身的两侧，其中中间两个腿相对机身向外更突出，且每个行走机构由一个直流减速电机驱动 参考文献 X-RHex Lite：能跳跃爬楼的六足机器人 看那些已经入侵生活的机器人","categories":[{"name":"Programming","slug":"Programming","permalink":"https://baidut.github.io/categories/Programming/"}],"tags":[{"name":"robot","slug":"robot","permalink":"https://baidut.github.io/tags/robot/"},{"name":"C","slug":"C","permalink":"https://baidut.github.io/tags/C/"}]}]}