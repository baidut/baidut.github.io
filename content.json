{"meta":{"title":"BitBlog","subtitle":"1 Bit = 1.1102e-16 Pebibyte","description":"Stay Hungry,Stay Foolish","author":"Zhenqiang Ying","url":"https://baidut.github.io"},"pages":[{"title":"Publications","date":"2017-05-20T10:18:38.000Z","updated":"2017-05-20T10:19:50.825Z","comments":true,"path":"publications/index.html","permalink":"https://baidut.github.io/publications/index.html","excerpt":"","text":"2017Z. Ying, G. Li, S. Wen, and G. TanORGB: offset correction in rgb color space for illumination-robust image processingICASSP 2017 Poster 2016Z. Ying, G. Li, X. Zang, R. Wang, and W. WangA novel shadow-free feature extractor for real-time road detectionACM MM 2016 Z. Ying and G. LiRobust lane marking detection using boundary-based inverse perspective mappingICASSP 2016 PDF Poster 2015Z. Ying, G. Li, and G. TanAn illumination-robust approach for feature-based road detectionISM 2015 PDF Code"},{"title":"categories","date":"2017-05-20T14:04:34.000Z","updated":"2017-09-04T06:55:24.201Z","comments":false,"path":"categories/index.html","permalink":"https://baidut.github.io/categories/index.html","excerpt":"","text":""},{"title":"About Me","date":"2017-05-20T10:28:52.000Z","updated":"2017-05-21T01:17:57.831Z","comments":true,"path":"about/index-old.html","permalink":"https://baidut.github.io/about/index-old.html","excerpt":"","text":"Download Resume Short Curriculum Vitae Since 08.2015 - master student at Peking University, China 08.2011 - 07.2015 - student at Dalian University of Technology, China Awards, Grants, Scholarships2016 Merit Student Pacesetter, Peking University Exceptional Award for Academic innovation, Peking University National Scholarship, Department of Education of China 2015 Outstanding Undergraduate, Dalian University of Technology Research interestsMy research interests are in Computer Vision and Machine Learning.I am especially interested in “physics-based” and “bio-inspired” approaches for computer vision tasks. Publications2017Z. Ying, G. Li, S. Wen, and G. TanORGB: offset correction in rgb color space for illumination-robust image processingICASSP 2017 Poster 2016Z. Ying, G. Li, X. Zang, R. Wang, and W. WangA novel shadow-free feature extractor for real-time road detectionACM MM 2016 Z. Ying and G. LiRobust lane marking detection using boundary-based inverse perspective mappingICASSP 2016 PDF Poster 2015Z. Ying, G. Li, and G. TanAn illumination-robust approach for feature-based road detectionISM 2015 PDF Code"},{"title":"tags","date":"2017-05-20T12:13:41.000Z","updated":"2017-09-04T06:55:02.872Z","comments":false,"path":"tags/index.html","permalink":"https://baidut.github.io/tags/index.html","excerpt":"","text":""},{"title":"Zhenqiang Ying's Resume","date":"2017-09-04T05:06:11.000Z","updated":"2017-09-04T14:43:16.400Z","comments":true,"path":"about/index.html","permalink":"https://baidut.github.io/about/index.html","excerpt":"","text":"Download Resume Research Interests Computer Vision Computer Graphics Artificial Intelligence Education Since 08.2015M.S. in Computer Science @ Peking UniversityAdvisor: Prof. Ge Li, GPA: 3.88, Ranking: 1/77 08/2011 - 07/2015B.S. in Computer Science @ Dalian University of TechnologyGPA: 3.66, Ranking: 4/90 PublicationsZ. Ying, G. Li, S. Wen, and G. TanORGB: offset correction in rgb color space for illumination-robust image processingICASSP 2017 Poster Z. Ying, G. Li, X. Zang, R. Wang, and W. WangA novel shadow-free feature extractor for real-time road detectionACM MM 2016 Z. Ying and G. LiRobust lane marking detection using boundary-based inverse perspective mappingICASSP 2016 PDF Poster Z. Ying, G. Li, and G. TanAn illumination-robust approach for feature-based road detectionISM 2015 PDF Code Awards 2016 Merit Student Pacesetter, Peking University Exceptional Award for Academic innovation, Peking University National Scholarship, Department of Education of China 2015 Outstanding Undergraduate, Dalian University of Technology Links google scholar github linkedin"}],"posts":[{"title":"A New Underwater Image Enhancing Method via Color Correction and Illumination Adjustment","slug":"vcip2017underwater","date":"2017-08-31T16:00:00.000Z","updated":"2017-09-04T14:22:57.146Z","comments":true,"path":"2017/09/01/vcip2017underwater/","link":"","permalink":"https://baidut.github.io/2017/09/01/vcip2017underwater/","excerpt":"Since the light will be absorbed and scattered when travels in water, underwater imaging exists three major difficulties, including color cast, under-exposure, and fuzz. The solutions to overcome those issues are important for the exploration of the ocean. In this paper, we propose a new algorithm for improving the quality of underwater images. The algorithm is composed of two components: color correction and illumination adjustment. First, we use an efficient color enhancement method to solve the color cast. Then, based on Retinex model, we make the illumination adjustment, mainly extracting the illumination map and implementing gamma correction on it successively. Experimental results show that visual performance of our method outperforms that of other methods, and processing complexity is relatively simpler.","text":"Since the light will be absorbed and scattered when travels in water, underwater imaging exists three major difficulties, including color cast, under-exposure, and fuzz. The solutions to overcome those issues are important for the exploration of the ocean. In this paper, we propose a new algorithm for improving the quality of underwater images. The algorithm is composed of two components: color correction and illumination adjustment. First, we use an efficient color enhancement method to solve the color cast. Then, based on Retinex model, we make the illumination adjustment, mainly extracting the illumination map and implementing gamma correction on it successively. Experimental results show that visual performance of our method outperforms that of other methods, and processing complexity is relatively simpler. vcip2017underwater","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"image processing","slug":"image-processing","permalink":"https://baidut.github.io/tags/image-processing/"},{"name":"image enhancement","slug":"image-enhancement","permalink":"https://baidut.github.io/tags/image-enhancement/"}]},{"title":"A Reduced-reference Color Distortion Measurement Scheme for Enhanced Low-light Images","slug":"vcip2017color","date":"2017-08-31T16:00:00.000Z","updated":"2017-09-04T14:26:13.447Z","comments":true,"path":"2017/09/01/vcip2017color/","link":"","permalink":"https://baidut.github.io/2017/09/01/vcip2017color/","excerpt":"Low-light image enhancement algorithms can improve the subjective visual quality of low-light images and support the extraction of valuable information for some computer vision techniques. Although many low-light image enhancement algorithms have been developed in recent years, the assessment methods for enhanced images are still open topics. In this paper, we use multi-exposure image sequences and a camera response model to calculate the color distortion of enhanced images. Specifically, we first select the useful information from a multi-exposure image sequence to form a reference image based on the illumination of the enhanced image. Then, we calculate the exposure ratio map between the reference image and the enhanced image for each color channel using the camera response model. Finally, the color distortion is calculated based on the difference between three color channel exposure ratios. Experiment results show that the Pearson’s linear correction coefficient (PLCC) and Spearman’s rank correlation coefficient (SRCC) between the results of our reduced-reference method and that of the full-reference CIE Lab color difference method (∆Eab ) are close to 1. Meanwhile, our method is significantly superior to the existing enhanced image color metrics.","text":"Low-light image enhancement algorithms can improve the subjective visual quality of low-light images and support the extraction of valuable information for some computer vision techniques. Although many low-light image enhancement algorithms have been developed in recent years, the assessment methods for enhanced images are still open topics. In this paper, we use multi-exposure image sequences and a camera response model to calculate the color distortion of enhanced images. Specifically, we first select the useful information from a multi-exposure image sequence to form a reference image based on the illumination of the enhanced image. Then, we calculate the exposure ratio map between the reference image and the enhanced image for each color channel using the camera response model. Finally, the color distortion is calculated based on the difference between three color channel exposure ratios. Experiment results show that the Pearson’s linear correction coefficient (PLCC) and Spearman’s rank correlation coefficient (SRCC) between the results of our reduced-reference method and that of the full-reference CIE Lab color difference method (∆Eab ) are close to 1. Meanwhile, our method is significantly superior to the existing enhanced image color metrics.","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"image processing","slug":"image-processing","permalink":"https://baidut.github.io/tags/image-processing/"},{"name":"image Enhancement","slug":"image-Enhancement","permalink":"https://baidut.github.io/tags/image-Enhancement/"}]},{"title":"A New Shadow Removal Method using Color-Lines","slug":"caip2017deshadow","date":"2017-08-21T16:00:00.000Z","updated":"2017-09-04T08:05:53.962Z","comments":true,"path":"2017/08/22/caip2017deshadow/","link":"","permalink":"https://baidut.github.io/2017/08/22/caip2017deshadow/","excerpt":"In this paper, we present a novel method for single-image shadow removal. From the observation of images with shadow, we find that the pixels from the object with same material will form a line in the RGB color space as illumination changes. Besides, we find these lines do not cross with the origin due to the effect of ambient light. Thus, we establish an offset correction relationship to remove the effect of ambient light. Then we derive a linear shadow image model to perform color-line identification. With the linear model, our shadow removal method is proposed as following. First, perform color-line clustering and illumination estimation. Second, use an on-the-fly learning method to detect umbra and penumbra. Third, estimate the shadow scale by the statistics of shadow-free regions. Finally, refine the shadow scale by illumination optimization. Our method is simple and effective for producing high-quality shadow-free images and has the ability for processing scenes with rich texture types and non-uniform shadows.","text":"In this paper, we present a novel method for single-image shadow removal. From the observation of images with shadow, we find that the pixels from the object with same material will form a line in the RGB color space as illumination changes. Besides, we find these lines do not cross with the origin due to the effect of ambient light. Thus, we establish an offset correction relationship to remove the effect of ambient light. Then we derive a linear shadow image model to perform color-line identification. With the linear model, our shadow removal method is proposed as following. First, perform color-line clustering and illumination estimation. Second, use an on-the-fly learning method to detect umbra and penumbra. Third, estimate the shadow scale by the statistics of shadow-free regions. Finally, refine the shadow scale by illumination optimization. Our method is simple and effective for producing high-quality shadow-free images and has the ability for processing scenes with rich texture types and non-uniform shadows. 12345678@inproceedings&#123;yu2017new, title=&#123;A New Shadow Removal Method Using Color-Lines&#125;, author=&#123;Yu, Xiaoming and Li, Ge and Ying, Zhenqiang and Guo, Xiaoqiang&#125;, booktitle=&#123;International Conference on Computer Analysis of Images and Patterns&#125;, pages=&#123;307--319&#125;, year=&#123;2017&#125;, organization=&#123;Springer&#125;&#125; Draft version of the paper can be found at ResearchGate:","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"image processing","slug":"image-processing","permalink":"https://baidut.github.io/tags/image-processing/"},{"name":"shadow removal","slug":"shadow-removal","permalink":"https://baidut.github.io/tags/shadow-removal/"}]},{"title":"A New Image Contrast Enhancement Algorithm using Exposure Fusion Framework","slug":"caip2017fuse2","date":"2017-08-21T16:00:00.000Z","updated":"2017-09-04T08:05:56.307Z","comments":true,"path":"2017/08/22/caip2017fuse2/","link":"","permalink":"https://baidut.github.io/2017/08/22/caip2017fuse2/","excerpt":"Low-light images are not conducive to human observation and computer vision algorithms due to their low visibility. Although many image enhancement techniques have been proposed to solve this problem, existing methods inevitably introduce contrast under- and over-enhancement. In this paper, we propose an exposure fusion framework and an enhancement algorithm to provide an accurate contrast enhancement.","text":"Low-light images are not conducive to human observation and computer vision algorithms due to their low visibility. Although many image enhancement techniques have been proposed to solve this problem, existing methods inevitably introduce contrast under- and over-enhancement. In this paper, we propose an exposure fusion framework and an enhancement algorithm to provide an accurate contrast enhancement. Specifically, we first design the weight matrix for image fusion using illumination estimation techniques. Then we introduce our camera response model to synthesize multi-exposure images. Next, we find the best exposure ratio so that the synthetic image is well-exposed in the regions where the original image under-exposed. Finally, the input image and the synthetic image are fused according to the weight matrix to obtain the enhancement result. Experiments show that our method can obtain results with less contrast and lightness distortion compared to that of several state-of-the-art methods. 12345678@inproceedings&#123;ying2017new, title=&#123;A New Image Contrast Enhancement Algorithm Using Exposure Fusion Framework&#125;, author=&#123;Ying, Zhenqiang and Li, Ge and Ren, Yurui and Wang, Ronggang and Wang, Wenmin&#125;, booktitle=&#123;International Conference on Computer Analysis of Images and Patterns&#125;, pages=&#123;36--46&#125;, year=&#123;2017&#125;, organization=&#123;Springer&#125;&#125; Source code and supplemental materials can be found at our project website Draft version of the paper can be found at ResearchGate:","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"image processing","slug":"image-processing","permalink":"https://baidut.github.io/tags/image-processing/"},{"name":"image enhancement","slug":"image-enhancement","permalink":"https://baidut.github.io/tags/image-enhancement/"}]},{"title":"ORGB：Offset correction in RGB Color Space for Illumination-Robust Image Processing","slug":"icassp2017orgb","date":"2017-03-04T16:00:00.000Z","updated":"2017-09-04T13:20:10.068Z","comments":true,"path":"2017/03/05/icassp2017orgb/","link":"","permalink":"https://baidut.github.io/2017/03/05/icassp2017orgb/","excerpt":"Single materials have colors which form straight lines in RGB space. However, in severe shadow cases, those lines do not intersect the origin, which is inconsistent with the description of most literature. This paper is concerned with the detection and correction of the offset between the intersection and origin. First, we analyze the reason for forming that offset via an optical imaging model. Second, we present a simple and effective way to detect and remove the offset. The resulting images, named ORGB, have almost the same appearance as the original RGB images while are more illumination-robust for color space conversion. Besides, image processing using ORGB instead of RGB is free from the interference of shadows. Finally, the proposed offset correction method is ap- plied to road detection task, improving the performance both in quantitative and qualitative evaluations.","text":"Single materials have colors which form straight lines in RGB space. However, in severe shadow cases, those lines do not intersect the origin, which is inconsistent with the description of most literature. This paper is concerned with the detection and correction of the offset between the intersection and origin. First, we analyze the reason for forming that offset via an optical imaging model. Second, we present a simple and effective way to detect and remove the offset. The resulting images, named ORGB, have almost the same appearance as the original RGB images while are more illumination-robust for color space conversion. Besides, image processing using ORGB instead of RGB is free from the interference of shadows. Finally, the proposed offset correction method is ap- plied to road detection task, improving the performance both in quantitative and qualitative evaluations. 12345678@inproceedings&#123;ying2017orgb, title=&#123;ORGB: Offset correction in RGB color space for illumination-robust image processing&#125;, author=&#123;Ying, Zhenqiang and Li, Ge and Wen, Sixin and Tan, Guozhen&#125;, booktitle=&#123;Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on&#125;, pages=&#123;1557--1561&#125;, year=&#123;2017&#125;, organization=&#123;IEEE&#125;&#125; project website arxiv","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"shadow removal","slug":"shadow-removal","permalink":"https://baidut.github.io/tags/shadow-removal/"},{"name":"image analysis","slug":"image-analysis","permalink":"https://baidut.github.io/tags/image-analysis/"}]},{"title":"Searching Action Proposals via Spatial Actionness Estimation and Temporal Path Inference and Tracking","slug":"accv2016","date":"2016-11-19T16:00:00.000Z","updated":"2017-09-04T13:40:01.881Z","comments":true,"path":"2016/11/20/accv2016/","link":"","permalink":"https://baidut.github.io/2016/11/20/accv2016/","excerpt":"In this paper, we address the problem of searching action proposals in unconstrained video clips. Our approach starts from actionness estimation on frame-level bounding boxes, and then aggregates the bounding boxes belonging to the same actor across frames via linking, associating, tracking to generate spatial-temporal continuous action paths. To achieve the target, a novel actionness estimation method is firstly proposed by utilizing both human appearance and motion cues. Then, the association of the action paths is formulated as a maximum set coverage problem with the results of actionness estimation as a priori as in [1]. To further promote the performance, we design an improved optimization objective for the problem and provide a greedy search algorithm to solve it. Finally, a tracking-by-detection scheme is designed to further refine the searched action paths. Extensive experiments on two challenging datasets, UCF-Sports and UCF-101, show that the proposed approach advances state-of-the-art proposal generation performance in terms of both accuracy and proposal quantity.","text":"In this paper, we address the problem of searching action proposals in unconstrained video clips. Our approach starts from actionness estimation on frame-level bounding boxes, and then aggregates the bounding boxes belonging to the same actor across frames via linking, associating, tracking to generate spatial-temporal continuous action paths. To achieve the target, a novel actionness estimation method is firstly proposed by utilizing both human appearance and motion cues. Then, the association of the action paths is formulated as a maximum set coverage problem with the results of actionness estimation as a priori as in [1]. To further promote the performance, we design an improved optimization objective for the problem and provide a greedy search algorithm to solve it. Finally, a tracking-by-detection scheme is designed to further refine the searched action paths. Extensive experiments on two challenging datasets, UCF-Sports and UCF-101, show that the proposed approach advances state-of-the-art proposal generation performance in terms of both accuracy and proposal quantity. 12345678@inproceedings&#123;li2016searching, title=&#123;Searching Action Proposals via Spatial Actionness Estimation and Temporal Path Inference and Tracking&#125;, author=&#123;Li, Nannan and Xu, Dan and Ying, Zhenqiang and Li, Zhihao and Li, Ge&#125;, booktitle=&#123;Asian Conference on Computer Vision&#125;, pages=&#123;384--399&#125;, year=&#123;2016&#125;, organization=&#123;Springer&#125;&#125; arXiv The framework of our action proposal generation approach","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"action recognition","slug":"action-recognition","permalink":"https://baidut.github.io/tags/action-recognition/"},{"name":"machine learning","slug":"machine-learning","permalink":"https://baidut.github.io/tags/machine-learning/"}]},{"title":"A Novel Shadow-Free Feature Extractor for Real-Time Road Detection","slug":"mm2016","date":"2016-08-21T16:00:00.000Z","updated":"2017-09-04T13:27:05.012Z","comments":true,"path":"2016/08/22/mm2016/","link":"","permalink":"https://baidut.github.io/2016/08/22/mm2016/","excerpt":"Road detection is one of the most important research areas in driver assistance and automated driving field. However, the performance of existing methods is still unsatisfactory, especially in severe shadow conditions. To overcome those difficulties, first we propose a novel shadow-free feature extractor based on the color distribution of road surface pixels. Then we present a road detection framework based on the extractor, whose performance is more accurate and robust than that of existing extractors. Also, the proposed framework has much low-complexity, which is suitable for usage in practical systems.","text":"Road detection is one of the most important research areas in driver assistance and automated driving field. However, the performance of existing methods is still unsatisfactory, especially in severe shadow conditions. To overcome those difficulties, first we propose a novel shadow-free feature extractor based on the color distribution of road surface pixels. Then we present a road detection framework based on the extractor, whose performance is more accurate and robust than that of existing extractors. Also, the proposed framework has much low-complexity, which is suitable for usage in practical systems. 12345678@inproceedings&#123;ying2016novel, title=&#123;A Novel Shadow-Free Feature Extractor for Real-Time Road Detection&#125;, author=&#123;Ying, Zhenqiang and Li, Ge and Zang, Xianghao and Wang, Ronggang and Wang, Wenmin&#125;, booktitle=&#123;Proceedings of the 2016 ACM on Multimedia Conference&#125;, pages=&#123;611--615&#125;, year=&#123;2016&#125;, organization=&#123;ACM&#125;&#125;","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"shadow removal","slug":"shadow-removal","permalink":"https://baidut.github.io/tags/shadow-removal/"},{"name":"image analysis","slug":"image-analysis","permalink":"https://baidut.github.io/tags/image-analysis/"}]},{"title":"Robust Lane Marking Detection using Boundary-Based Inverse Perspective Mapping","slug":"icassp2016bird","date":"2016-03-19T16:00:00.000Z","updated":"2017-09-04T13:44:25.479Z","comments":true,"path":"2016/03/20/icassp2016bird/","link":"","permalink":"https://baidut.github.io/2016/03/20/icassp2016bird/","excerpt":"Road detection, which brings a visual perceptive ability to vehicles, is essential to build driver assistance systems. To help detect lane markings in challenging scenarios, one-time calibration of inverse perspective mapping (IPM) parameters is employed to build a bird’s eye view of the road image. We propose an automatic IPM method based on road boundaries called BIRD (Boundary-based IPM for Road Detection), avoiding common problems of fixed IPM. Furthermore, integrating top-down and bottom-up attention, an illumination-robust lane marking detection approach using BIRD is proposed.","text":"Road detection, which brings a visual perceptive ability to vehicles, is essential to build driver assistance systems. To help detect lane markings in challenging scenarios, one-time calibration of inverse perspective mapping (IPM) parameters is employed to build a bird’s eye view of the road image. We propose an automatic IPM method based on road boundaries called BIRD (Boundary-based IPM for Road Detection), avoiding common problems of fixed IPM. Furthermore, integrating top-down and bottom-up attention, an illumination-robust lane marking detection approach using BIRD is proposed. 12345678@inproceedings&#123;ying2016robust, title=&#123;Robust lane marking detection using boundary-based inverse perspective mapping&#125;, author=&#123;Ying, Zhenqiang and Li, Ge&#125;, booktitle=&#123;Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on&#125;, pages=&#123;1921--1925&#125;, year=&#123;2016&#125;, organization=&#123;IEEE&#125;&#125;","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"shadow removal","slug":"shadow-removal","permalink":"https://baidut.github.io/tags/shadow-removal/"},{"name":"image analysis","slug":"image-analysis","permalink":"https://baidut.github.io/tags/image-analysis/"},{"name":"road detection","slug":"road-detection","permalink":"https://baidut.github.io/tags/road-detection/"}]},{"title":"An Illumination-Robust Approach for Feature-Based Road Detection","slug":"ism2015","date":"2015-12-13T16:00:00.000Z","updated":"2017-09-04T13:46:47.007Z","comments":true,"path":"2015/12/14/ism2015/","link":"","permalink":"https://baidut.github.io/2015/12/14/ism2015/","excerpt":"Road detection algorithms constitute a basis for intelligent vehicle systems which are designed to improve safety and efficiency for human drivers. In this paper, a novel road detection approach intended for tackling illumination-related effects is proposed. First, a grayscale image of modified saturation is derived from the input color image during preprocessing, effectively diminishing cast shadows. Second, the road boundary lines are detected, which provides an adaptive region of interest for the following lane-marking detection. Finally, an improved feature-based method is employed to identify lane-markings from the shadows. The experimental results show that the proposed approach is robust against illumination-related effects..","text":"Road detection algorithms constitute a basis for intelligent vehicle systems which are designed to improve safety and efficiency for human drivers. In this paper, a novel road detection approach intended for tackling illumination-related effects is proposed. First, a grayscale image of modified saturation is derived from the input color image during preprocessing, effectively diminishing cast shadows. Second, the road boundary lines are detected, which provides an adaptive region of interest for the following lane-marking detection. Finally, an improved feature-based method is employed to identify lane-markings from the shadows. The experimental results show that the proposed approach is robust against illumination-related effects.. 12345678@inproceedings&#123;ying2015illumination, title=&#123;An illumination-robust approach for feature-based road detection&#125;, author=&#123;Ying, Zhenqiang and Li, Ge and Tan, Guozhen&#125;, booktitle=&#123;Multimedia (ISM), 2015 IEEE International Symposium on&#125;, pages=&#123;278--281&#125;, year=&#123;2015&#125;, organization=&#123;IEEE&#125;&#125;","categories":[{"name":"Publications","slug":"Publications","permalink":"https://baidut.github.io/categories/Publications/"}],"tags":[{"name":"image analysis","slug":"image-analysis","permalink":"https://baidut.github.io/tags/image-analysis/"},{"name":"road detection","slug":"road-detection","permalink":"https://baidut.github.io/tags/road-detection/"}]},{"title":"JPEGHexViewer","slug":"JPEGParser","date":"2015-01-30T16:00:00.000Z","updated":"2017-09-04T14:53:01.038Z","comments":true,"path":"2015/01/31/JPEGParser/","link":"","permalink":"https://baidut.github.io/2015/01/31/JPEGParser/","excerpt":"","text":"The JPEGHexViewer is a tool to help you figure out the syntax structure of JPEG file. It is written in C++, using Qt (Qt5) framework. Source Code","categories":[{"name":"Projects","slug":"Projects","permalink":"https://baidut.github.io/categories/Projects/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://baidut.github.io/tags/C/"}]},{"title":"Simulate the alternate tripod gait using Webots","slug":"alternate-tripod-gait","date":"2014-12-05T16:00:00.000Z","updated":"2017-09-04T15:25:08.368Z","comments":true,"path":"2014/12/06/alternate-tripod-gait/","link":"","permalink":"https://baidut.github.io/2014/12/06/alternate-tripod-gait/","excerpt":"","text":"本文介绍三脚架对换式步态 （alternate tripod gait）的实现 1234┌───────────┐ ^ right│ 0 1 2 │ │ │ 3 4 5 │ front &lt;──┘└───────────┘ 一侧的前后两足和另一侧的中间足构成一个三脚架，每一时刻有一个三脚架处于支撑相，另一个三脚架出于摆动相，每隔半个周期进行切换。 足沿着轴摆动时，我们把处于支撑相的这段叫做小摆(smallSwap)，把处于摆动相的这段叫做大摆(bigSwap)。 123#define RATE 1.0/6 // 小摆占圆周的比率#define smallSwap 2 * M_PI * RATE#define bigSwap 2 * M_PI * (1-RATE) 步态涉及两个状态，一个是奇数号三脚架大摆，偶数号三脚架小摆；另一个是奇数号三脚架小摆，偶数号三脚架大摆。 1234const double posSwap [NUM_STATES][NUM_SERVOS] = &#123; &#123;smallSwap ,bigSwap ,smallSwap ,bigSwap ,smallSwap ,bigSwap &#125;, &#123;bigSwap ,smallSwap ,bigSwap ,smallSwap ,bigSwap ,smallSwap&#125;&#125;; 初始时六足偶数号在前（进入支撑相），奇数号在后（离开地面进入摆动相）。准备进入第一个状态。 1234double pos[NUM_SERVOS] = &#123; -smallSwap/2,+smallSwap/2,-smallSwap/2, +smallSwap/2,-smallSwap/2,+smallSwap/2&#125;; 主循环每隔一定周期调整一次各个足的位置，进入下一个状态。 1234567891011while(1) &#123;#define AJUST_PERIOD 50 elapsed++; if(elapsed % AJUST_PERIOD!=0) continue; state = !state; for (i = 0; i &lt; NUM_SERVOS; i++ )&#123; servo_set_position(servos[i], pos[i]); pos[i] += posSwap[state][i]; &#125;&#125; 调整两侧足的运行方向，两侧足同向后运行，就可以实现倒退；一侧足向前，一侧足向后，就可以实现原地转弯。 1234const int goForward [] = &#123; 1, 1, 1, 1, 1, 1&#125;;const int goBackward [] = &#123;-1,-1,-1,-1,-1,-1&#125;;const int turnLeft [] = &#123; 1, 1, 1,-1,-1,-1&#125;;const int turnRight [] = &#123;-1,-1,-1, 1, 1, 1&#125;; 加上转向，完整的仿真程序如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#include &lt;webots/robot.h&gt;#include &lt;webots/servo.h&gt;#include &lt;stdio.h&gt;#define NUM_SERVOS 6#define NUM_STATES 2#define RATE 1.0/6#define smallSwap 2 * M_PI * RATE#define bigSwap 2 * M_PI * (1-RATE)#define TIME_STEP 16#define AJUST_PERIOD 50int main() &#123; const char *SERVO_NAMES[] = &#123; \"servo_r0\", \"servo_r1\", \"servo_r2\", \"servo_l0\", \"servo_l1\", \"servo_l2\" &#125;; WbDeviceTag servos[NUM_SERVOS]; const int goForward [] = &#123; 1, 1, 1, 1, 1, 1&#125;; const int goBackward [] = &#123;-1,-1,-1,-1,-1,-1&#125;; const int turnLeft [] = &#123; 1, 1, 1,-1,-1,-1&#125;; const int turnRight [] = &#123;-1,-1,-1, 1, 1, 1&#125;; const double posSwap [NUM_STATES][NUM_SERVOS] = &#123; &#123;smallSwap ,bigSwap ,smallSwap ,bigSwap ,smallSwap ,bigSwap &#125;, &#123;bigSwap ,smallSwap ,bigSwap ,smallSwap ,bigSwap ,smallSwap&#125; &#125;; double pos[NUM_SERVOS] = &#123; -smallSwap/2,+smallSwap/2,-smallSwap/2, +smallSwap/2,-smallSwap/2,+smallSwap/2 &#125;; int i; // 初始化Webots链接舵机 wb_robot_init(); for (i = 0; i &lt; NUM_SERVOS; i++) &#123; servos[i] = wb_robot_get_device(SERVO_NAMES[i]); if (!servos[i]) printf(\"could not find servo: %s\\n\",SERVO_NAMES[i]); &#125; const int* dir = turnRight;//turnLeft;//goBackward;// goForward; int state; int elapsed = 0; // 初始位置 for (i = 0; i &lt; NUM_SERVOS; i++ ) pos[i] = dir[i] * pos[i]; // 主循环 while(wb_robot_step(TIME_STEP)!=-1) &#123; elapsed++; if(elapsed % AJUST_PERIOD!=0) continue; state = (elapsed / AJUST_PERIOD + 1) % NUM_STATES; for (i = 0; i &lt; NUM_SERVOS; i++ )&#123; wb_servo_set_position(servos[i], pos[i]); pos[i] += dir[i] * posSwap[state][i]; &#125; &#125; wb_robot_cleanup(); return 0;&#125; 仿真结果如下：","categories":[{"name":"Programming","slug":"Programming","permalink":"https://baidut.github.io/categories/Programming/"}],"tags":[{"name":"robot","slug":"robot","permalink":"https://baidut.github.io/tags/robot/"}]},{"title":"Build a six-legged robot (RHEX)","slug":"rhexapod","date":"2014-12-05T16:00:00.000Z","updated":"2017-09-04T15:07:08.746Z","comments":true,"path":"2014/12/06/rhexapod/","link":"","permalink":"https://baidut.github.io/2014/12/06/rhexapod/","excerpt":"","text":"We built an RHEX (a six-legged robot inspired by hexapod cockroach). I put some notes taken when building our robot here just for helping me rewind the memory. First, let’s watch a video to know this interesting robot! Physical simulationAlternate tripod gait","categories":[{"name":"Projects","slug":"Projects","permalink":"https://baidut.github.io/categories/Projects/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://baidut.github.io/tags/C/"}]}]}